[["index.html", "ENGE 5714 Course Notes 2021 Preamble", " ENGE 5714 Course Notes 2021 Dr. Katz 2021-02-07 Preamble This book will be a living document of notes for ENGE 5714 - Quantitative Research Methods in Engineering Education. I will try to keep it updated and post alerts about updates in our course Slack workspace. "],["week-1-introductions.html", "Chapter1 Week 1: Introductions", " Chapter1 Week 1: Introductions In week one we just reviewed some of the materials from the fall semester. By the end we discussed R and RStudio, but this first week was primarily about getting to know each other and the outline of the course. "],["week-2-intro-stats-data-distributions-intro-r-rstudio.html", "Chapter2 Week 2: Intro stats, Data &amp; Distributions, Intro R &amp; RStudio 2.1 First steps in R 2.2 Getting your R environment set up 2.3 Reading in data 2.4 Exploring the data 2.5 Plotting data 2.6 Some brief stats", " Chapter2 Week 2: Intro stats, Data &amp; Distributions, Intro R &amp; RStudio This week, we discuss some very basic ideas related to statistics, data, and working in R. 2.1 First steps in R We can create a new variable by assigning it a value with the &lt;- operator. Lets create a vector of numbers 1 to 10 with the seq() function and then a separate vector that takes each of the x values, multiplies it by 2, and adds 3. x &lt;- seq(1:10) y &lt;- 2* x + 3 Just to make sure everything worked as expected, we can then just type x and y and R will print their values. We could also look in the environment window to see whether those variables (and their expected values) were actually created. x ## [1] 1 2 3 4 5 6 7 8 9 10 y ## [1] 5 7 9 11 13 15 17 19 21 23 So far, so good. If we want to quickly visualize this, we could create a simple scatter plot with the plot() command (note: we will come back to plotting data much more in week 3). plot(x, y) 2.2 Getting your R environment set up One of the first things you will have in any script or .rmd file is a section to load all the libraries that you use in that script. You can install a library by using the install.packages() function, for example: install.packages(\"tidyverse\"), install.packages(\"janitor\"), and install.packages(\"psych\") with this installed, you can then load the package using the library() function library(tidyverse) library(janitor) library(psych) 2.3 Reading in data A good first step when working in R is to check which directory you are working in with the getwd() function. You should get a directory in response. getwd() ## [1] &quot;C:/Users/akatz4/Desktop/test_course_note&quot; You can also check which files are in that directory with list.files(). list.files() ## [1] &quot;_book&quot; ## [2] &quot;_bookdown.yml&quot; ## [3] &quot;_bookdown_files&quot; ## [4] &quot;_output.yml&quot; ## [5] &quot;01-Week_01.Rmd&quot; ## [6] &quot;02-Week_02.Rmd&quot; ## [7] &quot;03-Week_03.Rmd&quot; ## [8] &quot;04-Week_04.Rmd&quot; ## [9] &quot;05-Week_05.Rmd&quot; ## [10] &quot;06-Week_06.Rmd&quot; ## [11] &quot;book.bib&quot; ## [12] &quot;docs&quot; ## [13] &quot;ENGE_5714_2021_pre_survey.csv&quot; ## [14] &quot;Free Reduced Lunch by Schools and Grade Structures 2008-2017_final.csv&quot; ## [15] &quot;index.Rmd&quot; ## [16] &quot;packages.bib&quot; ## [17] &quot;preamble.tex&quot; ## [18] &quot;README.md&quot; ## [19] &quot;RExam.dat&quot; ## [20] &quot;seniorsurvey.csv&quot; ## [21] &quot;style.css&quot; ## [22] &quot;survey_student_info.csv&quot; ## [23] &quot;test_course_note.Rproj&quot; ## [24] &quot;test_course_notes.Rmd&quot; ## [25] &quot;test_course_notes_files&quot; If you notice that the file you are looking for is not there, then you can use setwd() to change your working directory setwd(\"./Week 2/\") After that, make sure you have switched to the correct working directory getwd() and then list.files(). Assuming you have directed yourself to the correct place, you can now read in the file(s) that you want to be working with. There are a lot of ways to do this. Since we will be spending a lot of time in class working with .csv files, we will focus on using the read_csv() function from the readr package (part of the tidyverse collection of packages). This function will read in the .csv file and store the data as a tibble (a tidyverse version of a data frame, which we can think of as a collection of observations stored in rows with values for variables for each observation stored in columns). prior_survey &lt;- read_csv(&quot;ENGE_5714_2021_pre_survey.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## student_id = col_double() ## ) ## See spec(...) for full column specifications. 2.4 Exploring the data Now that we have loaded in the data, lets take a look at the csv. If we just run a line with the name of the tibble - i.e., prior_survey then we should receive a printout that shows the first several rows of that tibble and a listing of all the columns, along with the data types (i.e., double for numeric values, character for strings, etc) of each column. prior_survey ## # A tibble: 24 x 49 ## student_id `I have taken a~ `I am intereste~ `I know what a ~ `I know what a ~ `I know what a ~ `I know what a ~ ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Somewhat disagr~ Somewhat agree Strongly disagr~ Strongly disagr~ Somewhat disagr~ Strongly disagr~ ## 2 2 Strongly disagr~ Neither agree n~ Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 3 3 Strongly disagr~ Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 4 4 Somewhat disagr~ Strongly agree Strongly disagr~ Strongly disagr~ Somewhat agree Neither agree n~ ## 5 5 Somewhat agree Strongly agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 6 6 Somewhat disagr~ Somewhat agree Somewhat disagr~ Somewhat disagr~ Somewhat agree Somewhat agree ## 7 7 Strongly disagr~ Somewhat agree Strongly disagr~ Strongly disagr~ Somewhat agree Somewhat agree ## 8 8 Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 9 9 Strongly disagr~ Strongly agree Somewhat agree Somewhat agree Strongly agree Strongly agree ## 10 10 Neither agree n~ Strongly agree Somewhat agree Neither agree n~ Somewhat agree Somewhat agree ## # ... with 14 more rows, and 42 more variables: `I know what p-hacking means` &lt;chr&gt;, `I know what statistical power ## # means` &lt;chr&gt;, `I have heard of frequentist statistics before` &lt;chr&gt;, `I have heard of Bayesian statistics ## # before` &lt;chr&gt;, `I have heard the term &quot;parametric statistics&quot; before` &lt;chr&gt;, `I have heard the term ## # &quot;non-parametric statistics&quot; before` &lt;chr&gt;, `I know what a histogram is.` &lt;chr&gt;, `I know what a probability ## # distribution is.` &lt;chr&gt;, `I know what a random variable is.` &lt;chr&gt;, `I know what a probability distribution ## # function is.` &lt;chr&gt;, `I know what a cumulative distribution function is.` &lt;chr&gt;, `I know what the expectation of ## # a random variable is.` &lt;chr&gt;, `I know how to calculate the variance of a random variable.` &lt;chr&gt;, `I know what a ## # z score is.` &lt;chr&gt;, `I know how to calculate the correlation between two variables.` &lt;chr&gt;, `I know how to ## # interpret the correlation coefficient between two variables` &lt;chr&gt;, `I have heard of linear regression` &lt;chr&gt;, `I ## # know how to run a linear regression (in some software...or by hand, if I&#39;m feeling wild).` &lt;chr&gt;, `I know how to ## # interpret a linear regression.` &lt;chr&gt;, `I have heard of multiple regression` &lt;chr&gt;, `I know how to perform a ## # multiple regression` &lt;chr&gt;, `I know how to interpret a multiple regression` &lt;chr&gt;, `I have heard of logistic ## # regression.` &lt;chr&gt;, `I understand when to use a logistic regression.` &lt;chr&gt;, `I know how to interpret the results ## # of a logistic regression` &lt;chr&gt;, `I have heard of t-tests` &lt;chr&gt;, `I have performed a t-test before` &lt;chr&gt;, `I ## # know how to interpret the results of a t-test` &lt;chr&gt;, `I have heard of Analysis of Variance.` &lt;chr&gt;, `I ## # understand when to run an Analysis of Variance (ANOVA)` &lt;chr&gt;, `I know how to interpret the results from an ## # ANOVA` &lt;chr&gt;, `I have heard of a chi-square test` &lt;chr&gt;, `I have used a chi-square test before` &lt;chr&gt;, `I know ## # how to interpret the results of a chi-square test` &lt;chr&gt;, `I have heard of cluster analysis before` &lt;chr&gt;, `I ## # have used cluster analysis before` &lt;chr&gt;, `I know how to interpret the results of a cluster analysis` &lt;chr&gt;, `I ## # have heard of factor analysis (either exploratory or confirmatory)` &lt;chr&gt;, `I have used factor analysis (either ## # exploratory or confirmatory)` &lt;chr&gt;, `I know how to interpret the results of a factor analysis (either ## # exploratory or confirmatory)` &lt;chr&gt;, `I already have R and Rstudio downloaded to my computer.` &lt;chr&gt;, `I have ## # used R before` &lt;chr&gt; When we do this, we see that there are a bunch of columns that have spaces in their names. This is okay (in the sense that R can handle this), but it can be a little frustrating to work with. Lets try cleaning the column names with clean_names() from the janitor package. This function will replace the spaces in the column names with underscores and make everything lower case. So, a column name like I have take a statistics course before will be changed to i_have_taken_a_statistics_course_before. prior_survey &lt;- prior_survey %&gt;% clean_names() # from janitor package Look at the data in prior_survey again and see if anything looks different (hint: it should). prior_survey ## # A tibble: 24 x 49 ## student_id i_have_taken_a_~ i_am_interested~ i_know_what_a_t~ i_know_what_a_t~ i_know_what_a_s~ i_know_what_a_p~ ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Somewhat disagr~ Somewhat agree Strongly disagr~ Strongly disagr~ Somewhat disagr~ Strongly disagr~ ## 2 2 Strongly disagr~ Neither agree n~ Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 3 3 Strongly disagr~ Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 4 4 Somewhat disagr~ Strongly agree Strongly disagr~ Strongly disagr~ Somewhat agree Neither agree n~ ## 5 5 Somewhat agree Strongly agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 6 6 Somewhat disagr~ Somewhat agree Somewhat disagr~ Somewhat disagr~ Somewhat agree Somewhat agree ## 7 7 Strongly disagr~ Somewhat agree Strongly disagr~ Strongly disagr~ Somewhat agree Somewhat agree ## 8 8 Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 9 9 Strongly disagr~ Strongly agree Somewhat agree Somewhat agree Strongly agree Strongly agree ## 10 10 Neither agree n~ Strongly agree Somewhat agree Neither agree n~ Somewhat agree Somewhat agree ## # ... with 14 more rows, and 42 more variables: i_know_what_p_hacking_means &lt;chr&gt;, ## # i_know_what_statistical_power_means &lt;chr&gt;, i_have_heard_of_frequentist_statistics_before &lt;chr&gt;, ## # i_have_heard_of_bayesian_statistics_before &lt;chr&gt;, i_have_heard_the_term_parametric_statistics_before &lt;chr&gt;, ## # i_have_heard_the_term_non_parametric_statistics_before &lt;chr&gt;, i_know_what_a_histogram_is &lt;chr&gt;, ## # i_know_what_a_probability_distribution_is &lt;chr&gt;, i_know_what_a_random_variable_is &lt;chr&gt;, ## # i_know_what_a_probability_distribution_function_is &lt;chr&gt;, ## # i_know_what_a_cumulative_distribution_function_is &lt;chr&gt;, ## # i_know_what_the_expectation_of_a_random_variable_is &lt;chr&gt;, ## # i_know_how_to_calculate_the_variance_of_a_random_variable &lt;chr&gt;, i_know_what_a_z_score_is &lt;chr&gt;, ## # i_know_how_to_calculate_the_correlation_between_two_variables &lt;chr&gt;, ## # i_know_how_to_interpret_the_correlation_coefficient_between_two_variables &lt;chr&gt;, ## # i_have_heard_of_linear_regression &lt;chr&gt;, ## # i_know_how_to_run_a_linear_regression_in_some_software_or_by_hand_if_im_feeling_wild &lt;chr&gt;, ## # i_know_how_to_interpret_a_linear_regression &lt;chr&gt;, i_have_heard_of_multiple_regression &lt;chr&gt;, ## # i_know_how_to_perform_a_multiple_regression &lt;chr&gt;, i_know_how_to_interpret_a_multiple_regression &lt;chr&gt;, ## # i_have_heard_of_logistic_regression &lt;chr&gt;, i_understand_when_to_use_a_logistic_regression &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_of_a_logistic_regression &lt;chr&gt;, i_have_heard_of_t_tests &lt;chr&gt;, ## # i_have_performed_a_t_test_before &lt;chr&gt;, i_know_how_to_interpret_the_results_of_a_t_test &lt;chr&gt;, ## # i_have_heard_of_analysis_of_variance &lt;chr&gt;, i_understand_when_to_run_an_analysis_of_variance_anova &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_from_an_anova &lt;chr&gt;, i_have_heard_of_a_chi_square_test &lt;chr&gt;, ## # i_have_used_a_chi_square_test_before &lt;chr&gt;, i_know_how_to_interpret_the_results_of_a_chi_square_test &lt;chr&gt;, ## # i_have_heard_of_cluster_analysis_before &lt;chr&gt;, i_have_used_cluster_analysis_before &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_of_a_cluster_analysis &lt;chr&gt;, ## # i_have_heard_of_factor_analysis_either_exploratory_or_confirmatory &lt;chr&gt;, ## # i_have_used_factor_analysis_either_exploratory_or_confirmatory &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_of_a_factor_analysis_either_exploratory_or_confirmatory &lt;chr&gt;, ## # i_already_have_r_and_rstudio_downloaded_to_my_computer &lt;chr&gt;, i_have_used_r_before &lt;chr&gt; One other function that will we see more in the future is the table() function, which will create a table with the counts of the values for a variable. For example, if we wanted to quickly know how students answered the I have taken a quantitative research methods course before question, we can run the following: table(prior_survey$i_have_taken_a_quantitative_research_methods_course_before) ## ## Neither agree nor disagree Somewhat agree Somewhat disagree Strongly agree ## 2 5 5 2 ## Strongly disagree ## 10 2.5 Plotting data We will discuss plotting more next week, but here is a brief preview of whats to come There are multiple ways to plot data. Focusing on using ggplot, here are two. The first way passes the prior_survey dataframe explicitly to ggplot ggplot(data = prior_survey, mapping = aes(x = i_know_what_a_type_i_error_is)) + geom_bar() + coord_flip() The second way does this implicitly, using the pipe operator. Note that the results should be the same. prior_survey %&gt;% ggplot(mapping = aes(x = i_know_what_a_type_i_error_is)) + geom_bar() + coord_flip() If we wanted to get extra fancy, we could first convert the data from a wide format to a long format and then start plotting all the items together. Converting to long format would produce something like this: prior_survey %&gt;% gather(key = &quot;survey_item&quot;, value = &quot;survey_response&quot;) ## # A tibble: 1,176 x 2 ## survey_item survey_response ## &lt;chr&gt; &lt;chr&gt; ## 1 student_id 1 ## 2 student_id 2 ## 3 student_id 3 ## 4 student_id 4 ## 5 student_id 5 ## 6 student_id 6 ## 7 student_id 7 ## 8 student_id 8 ## 9 student_id 9 ## 10 student_id 10 ## # ... with 1,166 more rows Then we can combine that with the group_by() and summarize() functions and plot the results. prior_survey %&gt;% gather(key = &quot;survey_item&quot;, value = &quot;survey_response&quot;) %&gt;% group_by(survey_item, survey_response) %&gt;% summarize(n = n()) %&gt;% ggplot(mapping = aes(x = survey_response, y = survey_item, fill = n)) + geom_tile() ## `summarise()` regrouping output by &#39;survey_item&#39; (override with `.groups` argument) This plot is okay for giving a general sense of what is going on in these plots but there are a bunch of other ways to go about doing this. First, maybe we want to rename the response categories to a numerical scale. We can accomplish this with a mutate() and case_when(). prior_survey &lt;- prior_survey %&gt;% gather(key = &quot;survey_item&quot;, value = &quot;survey_response&quot;) %&gt;% mutate(survey_response_num = case_when(survey_response == &quot;Strongly disagree&quot; ~ 0, survey_response == &quot;Somewhat disagree&quot; ~ 1, survey_response == &quot;Neither agree nor disagree&quot; ~ 2, survey_response == &quot;Somewhat agree&quot; ~ 3, survey_response == &quot;Strongly agree&quot; ~ 4, )) Then we plot the same data but with the numerical scale along the x-axis. prior_survey %&gt;% group_by(survey_item, survey_response_num) %&gt;% summarize(n = n()) %&gt;% ggplot(mapping = aes(x = survey_response_num, y = survey_item, fill = n)) + geom_tile() ## `summarise()` regrouping output by &#39;survey_item&#39; (override with `.groups` argument) ## Warning: Removed 3 rows containing missing values (geom_tile). 2.6 Some brief stats In this weeks reading, there was also discussion about standard errors and the central limit theorem. These are fairly important theoretical concepts to grasp. To some extent they deal with the scenario where you go out and repeatedly sample from a population and calculate a statistic from each of those samples. The distributions of that statistic is what we will call the sampling distribution (as opposed to the sample distribution, which would more accurately describe the distribution of the data that we get in any one sample that we draw from the population). 2.6.1 Central Limit Theorem and Standard Error Demo - pop_students &lt;- 10000 zoom_min_pop &lt;- rnorm(n = pop_students, mean = 600, sd = 100) hist(zoom_min_pop) zoom_min_sample &lt;- sample(x = zoom_min_pop, size = 200, replace = FALSE) hist(zoom_min_sample) mean(zoom_min_sample) ## [1] 589.9368 sd(zoom_min_sample) ## [1] 100.1208 As a brief aside, lets review the idea of a loop num_reps &lt;- 100 data_vec &lt;- rep(NA, num_reps) # this creates an empty vector of size num_reps with NA in each entry # this loops through the vector starting at position 1 and ending at the final position (num_reps). For demonstration purposes, we replacing the NA in each entry with the number of that entry (i.e., the NA in the 20th entry is replaced with the number 20) for (i in 1:num_reps){ data_vec[i] &lt;- i } Okay, so thats how we create an empty vector and how we loop through its different entries. For this demo, we will also need to remenber how to generate random numbers from a norm distribution with a specified mean and standard deviation. rnorm(n = 10, mean = 5, sd = 2) # n is the number of random numbers we draw from this normal distribution ## [1] 4.3633405 3.9165137 4.7545297 2.4079111 3.9952598 5.4699150 4.9726253 0.1157952 4.7094517 3.8486615 Okay, so thats not bad. Now, that command will produce a vector with 10 random numbers. We can calculate the mean and standard deviation of those 10 numbers (which should be close to the values that we specified in rnorm() with the mean() and sd() functions. mean(rnorm(n = 10, mean = 5, sd = 2)) ## [1] 5.071956 sd(rnorm(n = 10, mean = 5, sd = 2)) ## [1] 3.46389 Next, lets act as if we are drawing a certain sample of size samp_size of data points for num_reps number of times. Keep in mind that, in practice, when we are collecting data ourselves in our own research, num_reps will almost always be 1. We are just demonstrating the underlying assumptions for how we can calculate some of the statistics that we use. num_reps &lt;- 1000 # specify how many times to take a sample samp_size &lt;- 200 # specify the size of each sample data_vec &lt;- rep(NA, num_reps) # create an empty vector of size num_reps with NA in each entry. for (i in 1:num_reps){ data_vec[i] &lt;- mean(rnorm(n = samp_size, mean = 600, sd = 100)) # store the mean of each of the num_rep samples } With this, we have a vector data_vec of size num_reps with the mean of each of our samples that we drew. This vector contains our sampling distribution of our sample means. NOTE: The standard deviation of this sampling mean is what we are calling our standard error. We can plot a histogram of this sampling distribution and calculate the standard deviation of the sampling mean. hist(data_vec) sd(data_vec) ## [1] 6.868112 On your own, try copying this code and changing the num_reps and sample_size variables to larger and smaller values. Focus on how the x-axis values in your histogram change when you change the num_reps and samp_size variables. Hint: CLT will explain the normal distribution of the sampling mean (the shape you see in the histogram) while the Weak Law of Large Numbers will explain the concentration around the true mean as samp_size increases (i.e., when we draw a larger sample size from the population, our sample mean gets closer to the population mean). ## Quick note on the rep() function: notice what happens when you specify &quot;each&quot; vs &quot;times&quot;. rep(c(1, 2), times = 5) ## [1] 1 2 1 2 1 2 1 2 1 2 rep(c(1, 2), each = 5) ## [1] 1 1 1 1 1 2 2 2 2 2 "],["week-3-data-cleaning-organizing-describing-and-communicating.html", "Chapter3 Week 3: Data Cleaning, Organizing, Describing, and Communicating 3.1 Visualizing your data 3.2 Joining two datasets 3.3 Discrete Predictor, Continuous Outcome 3.4 Continuous predictor and continuous outcome 3.5 Mutating Variables 3.6 Filtering and Selecting 3.7 Grouping and Summarizing", " Chapter3 Week 3: Data Cleaning, Organizing, Describing, and Communicating This week we focus on different steps you will often take when you first start working with your data. These tend to fall under the umbrella of data processing and often need to happen before you can start doing any kind of analysis. 3.1 Visualizing your data Once your data have been clearned, you are ready to start visualizing what you are working with. There is a huge range of what you can do with these plots. Thats great! On the other hand, it can quickly start to feel overwhelming. To help get this under control and make it more manageable, it is convenient to think about the types of data that you have. In particular, are your variables nominal, ordinal, interval, or ratio variables? 3.1.1 One continuous variable (either predictor or outcome variable) When you have one continuous variable, a standard option is to plot a histogram. These are plots that show the frequency of each of the values that the variable takes. Oftentimes it is helpful to create bins of values so that any number that falls in the 0-4 range counts in one bin, numbers from 5-9 are in a second bin, and so on. For this example, we will start by generating some data using rnorm(), which generates a random number (or in our case, num numbers) from a normal distribution with mean mu and standard deviation stdev. num &lt;- 50 mu &lt;- 5 stdev &lt;- 2 x &lt;- rnorm(n = num, mean = mu, sd = stdev) With these data generated, we can then quickly plot the histogram with hist(). This will use base R graphics. hist(x) You can also do this using ggplot rather than base R graphics. x_df &lt;- tibble(x_col = x) ggplot(data = x_df, mapping = aes(x = x_col)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The histogram is a very standard plot, and you should consider it a go-to option in your toolkit. Alternatively, you can use geom_density() instead of geom_histogram() to get a smooth graph rather than one with discrete bins. We will use the same data that we generated before. We will write this two ways to demnostrate how the pipe %&gt;% operator works. First way: ggplot(data = x_df, mapping = aes(x = x_col)) + geom_density() Second way: x_df %&gt;% ggplot(mapping = aes(x = x_col)) + geom_density() Just for fun, look at what happens to the the plot if you increase the sample size First, we will generate the data with a sample size of 5,000 rather than 50. num &lt;- 5000 mu &lt;- 5 stdev &lt;- 2 x &lt;- rnorm(n = num, mean = mu, sd = stdev) x_df &lt;- tibble(x_col = x) Then we will plot the histogram x_df %&gt;% ggplot(aes(x = x_col)) + geom_histogram() + labs(x = &quot;x value&quot;, y = &quot;Count&quot;, title = &quot;Histogram of normal distribution with n = 5000, mu = 5, sd = 2&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. And, finally, we can make the density plot instead of the histogram, if thats our jam. ggplot(data = x_df, mapping = aes(x = x_col)) + geom_density() 3.1.2 One Discrete Variable (either predictor or outcome) What if instead of a continuous (i.e., interval or ratio) variable we have a discrete variable such as a nominal (e.g., major, university) or ordinal (e.g., Likert scale item, level of education) variable? For that we can use something like geom_bar() or geom_col() to plot the counts of observations within each of those categories. To demonstrate this, we first need some data to work with. We will use the pre-semester, prior knowledge survey that everyone took. I have combined this years results with last years results in order to increase the sample size. After reading in the data, I will also use the clean_names() function from the janitor package. ## load in the data survey_df &lt;- read_csv(&quot;ENGE_5714_2021_pre_survey.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## student_id = col_double() ## ) ## See spec(...) for full column specifications. survey_df &lt;- survey_df %&gt;% clean_names() Next, we can go ahead and make a bar plot with the following code: survey_df %&gt;% ggplot(aes(x = i_have_taken_a_quantitative_research_methods_course_before)) + geom_bar() Notice that the ordering is not quite what we would want. It is alphabetical. Lets try to fix this. Here is one way: we first specify the levels of that variable (i.e., the different values that it could take) and store that in the variable q_levels. Then, we pass that to the factor() function, which will tell R that we want whichever variable is passed to factor() two things. First, it will say that we want to make that variable a factor variable with levels = .... Second, we set ordered = TRUE to tell R that there is a specific ordering to that variable. This way, whenever there is something like a plot that we make, the ordering will persist in the labeling and R will not show the labels in alphabetical order. Here is an example of that in action: q_levels &lt;- c(&quot;Strongly disagree&quot;, &quot;Somewhat disagree&quot;, &quot;Neither agree nor disagree&quot;, &quot;Somewhat agree&quot;, &quot;Strongly agree&quot;) survey_df$i_have_taken_a_quantitative_research_methods_course_before &lt;- factor(survey_df$i_have_taken_a_quantitative_research_methods_course_before, levels = q_levels, ordered = TRUE) Now try plotting these data. We will also add in a coord_flip() to plot the categories along the y-axis. This is a common move to avoid text from the different levels overlapping with each other. Finally, we will also change the x, y, and title labels with labs(). survey_df %&gt;% ggplot(aes(x = i_have_taken_a_quantitative_research_methods_course_before)) + geom_bar() + coord_flip() + labs(x = &quot;I have taken a quantitative research methods course before&quot;, y = &quot;Count&quot;, title = &quot;Reordered Example&quot;) 3.2 Joining two datasets Lets imagine that we have a separate dataset that has information about the students who completed the pre-course prior knowledge survey. First, we will load in that dataset survey_info_df &lt;- read_csv(&quot;survey_student_info.csv&quot;) ## Parsed with column specification: ## cols( ## student_id = col_double(), ## standing = col_character(), ## college = col_character(), ## required = col_character() ## ) Next, lets join the two datasets based on the student id column, which is in each of the two dataframes. survey_df &lt;- survey_df %&gt;% inner_join(survey_info_df, by = &quot;student_id&quot;) Now we should have both datasets joined into one and saved as survey_df. With this, we can make some nicer plots and do something like use facet_grid() to look at students who are masters and doctoral students, for example. survey_df %&gt;% ggplot(aes(x = i_have_taken_a_quantitative_research_methods_course_before)) + geom_bar() + facet_grid(standing ~.) + labs(x = &quot;I have taken a quantitative research methods course before&quot;, y = &quot;Count&quot;, title = &quot;Reordered Example&quot;) The x axis looks a little crowded. What if we try coord_flip()? survey_df %&gt;% filter(standing == &quot;doctoral&quot;) %&gt;% ggplot(aes(x = i_have_taken_a_quantitative_research_methods_course_before)) + geom_bar() + coord_flip() + facet_grid(standing ~.) + labs(x = &quot;I have taken a quantitative research methods course before&quot;, y = &quot;Count&quot;, title = &quot;Reordered Example&quot;) That looks much better. A quick note on filters If you want to look at only a subset of your data, you will want to use the filter() function. The general idea is that you can look at observations (rows) that match a certain criteria. For example, you may want to only look at students from a certain region or year or major. In our case, with the prior knowledge survey, lets say we only want to look at student who have to take the course (i.e., there is a yes for them for the required variable). We can do that with the first line. The second line just stores the result as a new dataframe called filtered_df. survey_df %&gt;% filter(required == &quot;yes&quot;) ## # A tibble: 12 x 52 ## student_id i_have_taken_a_~ i_am_interested~ i_know_what_a_t~ i_know_what_a_t~ i_know_what_a_s~ i_know_what_a_p~ ## &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Somewhat disagr~ Somewhat agree Strongly disagr~ Strongly disagr~ Somewhat disagr~ Strongly disagr~ ## 2 2 Strongly disagr~ Neither agree n~ Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 3 4 Somewhat disagr~ Strongly agree Strongly disagr~ Strongly disagr~ Somewhat agree Neither agree n~ ## 4 8 Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 5 9 Strongly disagr~ Strongly agree Somewhat agree Somewhat agree Strongly agree Strongly agree ## 6 11 Strongly disagr~ Strongly agree Strongly disagr~ Strongly disagr~ Strongly disagr~ Strongly disagr~ ## 7 16 Strongly agree Strongly agree Somewhat agree Somewhat agree Somewhat agree Somewhat agree ## 8 17 Strongly disagr~ Strongly agree Strongly disagr~ Strongly disagr~ Somewhat agree Somewhat agree ## 9 18 Somewhat disagr~ Somewhat agree Somewhat disagr~ Somewhat disagr~ Somewhat agree Somewhat agree ## 10 20 Strongly disagr~ Neither agree n~ Neither agree n~ Neither agree n~ Neither agree n~ Neither agree n~ ## 11 22 Strongly disagr~ Strongly agree Strongly disagr~ Strongly disagr~ Somewhat disagr~ Somewhat disagr~ ## 12 23 Somewhat agree Strongly agree Somewhat agree Somewhat agree Somewhat agree Strongly agree ## # ... with 45 more variables: i_know_what_p_hacking_means &lt;chr&gt;, i_know_what_statistical_power_means &lt;chr&gt;, ## # i_have_heard_of_frequentist_statistics_before &lt;chr&gt;, i_have_heard_of_bayesian_statistics_before &lt;chr&gt;, ## # i_have_heard_the_term_parametric_statistics_before &lt;chr&gt;, ## # i_have_heard_the_term_non_parametric_statistics_before &lt;chr&gt;, i_know_what_a_histogram_is &lt;chr&gt;, ## # i_know_what_a_probability_distribution_is &lt;chr&gt;, i_know_what_a_random_variable_is &lt;chr&gt;, ## # i_know_what_a_probability_distribution_function_is &lt;chr&gt;, ## # i_know_what_a_cumulative_distribution_function_is &lt;chr&gt;, ## # i_know_what_the_expectation_of_a_random_variable_is &lt;chr&gt;, ## # i_know_how_to_calculate_the_variance_of_a_random_variable &lt;chr&gt;, i_know_what_a_z_score_is &lt;chr&gt;, ## # i_know_how_to_calculate_the_correlation_between_two_variables &lt;chr&gt;, ## # i_know_how_to_interpret_the_correlation_coefficient_between_two_variables &lt;chr&gt;, ## # i_have_heard_of_linear_regression &lt;chr&gt;, ## # i_know_how_to_run_a_linear_regression_in_some_software_or_by_hand_if_im_feeling_wild &lt;chr&gt;, ## # i_know_how_to_interpret_a_linear_regression &lt;chr&gt;, i_have_heard_of_multiple_regression &lt;chr&gt;, ## # i_know_how_to_perform_a_multiple_regression &lt;chr&gt;, i_know_how_to_interpret_a_multiple_regression &lt;chr&gt;, ## # i_have_heard_of_logistic_regression &lt;chr&gt;, i_understand_when_to_use_a_logistic_regression &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_of_a_logistic_regression &lt;chr&gt;, i_have_heard_of_t_tests &lt;chr&gt;, ## # i_have_performed_a_t_test_before &lt;chr&gt;, i_know_how_to_interpret_the_results_of_a_t_test &lt;chr&gt;, ## # i_have_heard_of_analysis_of_variance &lt;chr&gt;, i_understand_when_to_run_an_analysis_of_variance_anova &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_from_an_anova &lt;chr&gt;, i_have_heard_of_a_chi_square_test &lt;chr&gt;, ## # i_have_used_a_chi_square_test_before &lt;chr&gt;, i_know_how_to_interpret_the_results_of_a_chi_square_test &lt;chr&gt;, ## # i_have_heard_of_cluster_analysis_before &lt;chr&gt;, i_have_used_cluster_analysis_before &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_of_a_cluster_analysis &lt;chr&gt;, ## # i_have_heard_of_factor_analysis_either_exploratory_or_confirmatory &lt;chr&gt;, ## # i_have_used_factor_analysis_either_exploratory_or_confirmatory &lt;chr&gt;, ## # i_know_how_to_interpret_the_results_of_a_factor_analysis_either_exploratory_or_confirmatory &lt;chr&gt;, ## # i_already_have_r_and_rstudio_downloaded_to_my_computer &lt;chr&gt;, i_have_used_r_before &lt;chr&gt;, standing &lt;chr&gt;, ## # college &lt;chr&gt;, required &lt;chr&gt; filtered_df &lt;- survey_df %&gt;% filter(required == &quot;yes&quot;) A little more about plotting We are going to shift gears again and look at a few different kinds of plots. The main thing to remember here is that you want to think about whether the variables you have are nominal, ordinal, or continuous (that includes interval and ratio). 3.3 Discrete Predictor, Continuous Outcome So far we have looked at plots for one variable, but of course we want to have ways to plot multiple variables simultaneously. We will start with the scneario where where we want to plot a continuous variable against a discrete variable. This can arise when you want to plot something like an assessment score and you think it may differ across groups in some way (maybe you intentionally introduced a difference by exposing the two groups to different interventions, for example). In these scenarios, a boxplot is a very standard way to go. To demonstrate this, we will simulate a situation in which we want to look at differences on an assessment. We are specificially interested in differences between chemistry and chemical engineering students. Lets go ahead and create the data by creating two groups of 20 students each. The chemical engineering students will have scores generated from a normal distribution with \\(\\mu = 85\\) and \\(\\sigma = 4\\) (i.e., a mean of 85 and a standard deviation of 4). We will say the chemistry students have scores from a normal distribution with \\(\\mu = 78\\) anad \\(\\sigma = 6\\). This about what these distributions might look like in your head. group_size &lt;- 20 chem_e_scores &lt;- rnorm(n = group_size, mean = 85, sd = 4) chem_scores &lt;- rnorm(n = group_size, mean = 78, sd = 6) data_df &lt;- tibble( discipline = rep(c(&quot;ChemE&quot;, &quot;Chemistry&quot;), each = group_size), score = c(chem_e_scores, chem_scores) ) With these data, we can then create a boxplot using geom_boxplot() data_df %&gt;% ggplot(aes(x = discipline, y = score)) + geom_boxplot() You can make a few modifications to possibly make this plot easier to read. The first is to put the discrete category on the y axis instead of the x axis. The second is to use geom_jitter() in addition to geom_boxplot() to show the individual points in each group. data_df %&gt;% ggplot(aes(y = score, x = discipline)) + geom_boxplot() + geom_jitter() 3.4 Continuous predictor and continuous outcome First, lets re-do a lot of the steps in this weeks script for reading in data and transforming it a little mydata &lt;- read_csv(&quot;Free Reduced Lunch by Schools and Grade Structures 2008-2017_final.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## div_num = col_double() ## ) ## See spec(...) for full column specifications. Check the structure of the data (this output is a bit long). str(mydata) ## tibble [2,101 x 137] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ sch_id : chr [1:2101] &quot;001-0070&quot; &quot;001-0080&quot; &quot;001-0530&quot; &quot;001-0540&quot; ... ## $ div_num : num [1:2101] 1 1 1 1 1 1 1 1 1 1 ... ## $ div_name : chr [1:2101] &quot;Accomack County&quot; &quot;Accomack County&quot; &quot;Accomack County&quot; &quot;Accomack County&quot; ... ## $ school_num : chr [1:2101] &quot;0070&lt;U+00A0&gt;&quot; &quot;0080&lt;U+00A0&gt;&quot; &quot;0530&lt;U+00A0&gt;&quot; &quot;0540&lt;U+00A0&gt;&quot; ... ## $ school_name : chr [1:2101] &quot;NANDUA HIGH&quot; &quot;CHINCOTEAGUE ELEM&quot; &quot;TANGIER COMBINED&quot; &quot;ARCADIA HIGH&quot; ... ## $ school_name2 : chr [1:2101] NA NA NA NA ... ## $ type0809 : chr [1:2101] &quot;SCH-HIGH&quot; &quot;SCH-ELEM&quot; &quot;SCH-COMB&quot; &quot;SCH-HIGH&quot; ... ## $ lowgrade_2008 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2008 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2008 : chr [1:2101] &quot;731&quot; &quot;263&quot; &quot;80&quot; &quot;638&quot; ... ## $ total_2008 : chr [1:2101] &quot;731&quot; &quot;263&quot; &quot;80&quot; &quot;638&quot; ... ## $ snp_0809 : chr [1:2101] &quot;659&quot; &quot;257&quot; &quot;80&quot; &quot;622&quot; ... ## $ free_elig_0809: chr [1:2101] &quot;306&quot; &quot;95&quot; &quot;38&quot; &quot;289&quot; ... ## $ free_per_0809 : chr [1:2101] &quot;46.43%&quot; &quot;36.96%&quot; &quot;47.50%&quot; &quot;46.46%&quot; ... ## $ red_elig_0809 : chr [1:2101] &quot;64&quot; &quot;8&quot; &quot;0&quot; &quot;56&quot; ... ## $ red_per_0809 : chr [1:2101] &quot;9.71%&quot; &quot;3.11%&quot; &quot;0.00%&quot; &quot;9.00%&quot; ... ## $ totalFRL_0809 : chr [1:2101] &quot;370&quot; &quot;103&quot; &quot;38&quot; &quot;345&quot; ... ## $ totalper_0809 : chr [1:2101] &quot;56.15%&quot; &quot;40.08%&quot; &quot;47.50%&quot; &quot;55.47%&quot; ... ## $ type0910 : chr [1:2101] &quot;SCH-HIGH&quot; &quot;SCH-ELEM&quot; &quot;SCH-COMB&quot; &quot;SCH-HIGH&quot; ... ## $ lowgrade_2009 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2009 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2009 : chr [1:2101] &quot;654&quot; &quot;266&quot; &quot;78&quot; &quot;634&quot; ... ## $ total_2009 : chr [1:2101] &quot;654&quot; &quot;266&quot; &quot;78&quot; &quot;634&quot; ... ## $ snp_0910 : chr [1:2101] &quot;655&quot; &quot;266&quot; &quot;78&quot; &quot;635&quot; ... ## $ free_elig_0910: chr [1:2101] &quot;290&quot; &quot;99&quot; &quot;36&quot; &quot;286&quot; ... ## $ free_per_0910 : chr [1:2101] &quot;44.27%&quot; &quot;37.22%&quot; &quot;46.15%&quot; &quot;45.04%&quot; ... ## $ red_elig_0910 : chr [1:2101] &quot;37&quot; &quot;14&quot; &quot;0&quot; &quot;66&quot; ... ## $ red_per_0910 : chr [1:2101] &quot;5.65%&quot; &quot;5.26%&quot; &quot;0.00%&quot; &quot;10.39%&quot; ... ## $ totalFRL_09010: chr [1:2101] &quot;327&quot; &quot;113&quot; &quot;36&quot; &quot;352&quot; ... ## $ totalper_0910 : chr [1:2101] &quot;49.92%&quot; &quot;42.48%&quot; &quot;46.15%&quot; &quot;55.43%&quot; ... ## $ type1011 : chr [1:2101] &quot;SCH-HIGH&quot; &quot;SCH-ELEM&quot; &quot;SCH-COMB&quot; &quot;SCH-HIGH&quot; ... ## $ lowgrade_2010 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2010 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2010 : chr [1:2101] &quot;603&quot; &quot;268&quot; &quot;74&quot; &quot;614&quot; ... ## $ total_2010 : chr [1:2101] &quot;603&quot; &quot;268&quot; &quot;74&quot; &quot;614&quot; ... ## $ snp_1011 : chr [1:2101] &quot;603&quot; &quot;277&quot; &quot;74&quot; &quot;606&quot; ... ## $ free_elig_1011: chr [1:2101] &quot;285&quot; &quot;108&quot; &quot;32&quot; &quot;308&quot; ... ## $ free_per_1011 : chr [1:2101] &quot;47.26%&quot; &quot;38.99%&quot; &quot;43.24%&quot; &quot;50.83%&quot; ... ## $ red_elig_1011 : chr [1:2101] &quot;46&quot; &quot;8&quot; &quot;0&quot; &quot;50&quot; ... ## $ red_per_1011 : chr [1:2101] &quot;7.63%&quot; &quot;2.89%&quot; &quot;0.00%&quot; &quot;8.25%&quot; ... ## $ totalFRL_1011 : chr [1:2101] &quot;331&quot; &quot;116&quot; &quot;32&quot; &quot;358&quot; ... ## $ totalper_1011 : chr [1:2101] &quot;54.89%&quot; &quot;41.88%&quot; &quot;43.24%&quot; &quot;59.08%&quot; ... ## $ type1112 : chr [1:2101] &quot;SCH-HIGH&quot; &quot;SCH-ELEM&quot; &quot;SCH-COMB&quot; &quot;SCH-HIGH&quot; ... ## $ lowgrade_2011 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2011 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2011 : chr [1:2101] &quot;593&quot; &quot;276&quot; &quot;73&quot; &quot;605&quot; ... ## $ total_2011 : chr [1:2101] &quot;593&quot; &quot;276&quot; &quot;73&quot; &quot;605&quot; ... ## $ snp_1112 : chr [1:2101] &quot;593&quot; &quot;281&quot; &quot;73&quot; &quot;611&quot; ... ## $ free_elig_1112: chr [1:2101] &quot;289&quot; &quot;116&quot; &quot;31&quot; &quot;318&quot; ... ## $ free_per_1112 : chr [1:2101] &quot;48.74%&quot; &quot;41.28%&quot; &quot;42.47%&quot; &quot;52.05%&quot; ... ## $ red_elig_1112 : chr [1:2101] &quot;50&quot; &quot;14&quot; &quot;0&quot; &quot;44&quot; ... ## $ red_per_1112 : chr [1:2101] &quot;8.43%&quot; &quot;4.98%&quot; &quot;0.00%&quot; &quot;7.20%&quot; ... ## $ totalFRL_1112 : chr [1:2101] &quot;339&quot; &quot;130&quot; &quot;31&quot; &quot;362&quot; ... ## $ totalper_1112 : chr [1:2101] &quot;57.17%&quot; &quot;46.26%&quot; &quot;42.47%&quot; &quot;59.25%&quot; ... ## $ type1213 : chr [1:2101] &quot;SCH-HIGH&quot; &quot;SCH-ELEM&quot; &quot;SCH-COMB&quot; &quot;SCH-HIGH&quot; ... ## $ lowgrade_2012 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2012 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2012 : chr [1:2101] &quot;637&quot; &quot;258&quot; &quot;68&quot; &quot;579&quot; ... ## $ total_2012 : chr [1:2101] &quot;637&quot; &quot;258&quot; &quot;68&quot; &quot;579&quot; ... ## $ snp_1213 : chr [1:2101] &quot;633&quot; &quot;259&quot; &quot;68&quot; &quot;579&quot; ... ## $ free_elig_1213: chr [1:2101] &quot;324&quot; &quot;117&quot; &quot;21&quot; &quot;348&quot; ... ## $ free_per_1213 : chr [1:2101] &quot;51.18%&quot; &quot;45.17%&quot; &quot;30.88%&quot; &quot;60.10%&quot; ... ## $ red_elig_1213 : chr [1:2101] &quot;42&quot; &quot;20&quot; &quot;5&quot; &quot;33&quot; ... ## $ red_per_1213 : chr [1:2101] &quot;6.64%&quot; &quot;7.72%&quot; &quot;7.35%&quot; &quot;5.70%&quot; ... ## $ totalFRL_1213 : chr [1:2101] &quot;366&quot; &quot;137&quot; &quot;26&quot; &quot;381&quot; ... ## $ totalper_1213 : chr [1:2101] &quot;57.82%&quot; &quot;52.90%&quot; &quot;38.24%&quot; &quot;65.80%&quot; ... ## $ type1314 : chr [1:2101] &quot;SCH-HIGH&quot; &quot;SCH-ELEM&quot; &quot;SCH-COMB&quot; &quot;SCH-HIGH&quot; ... ## $ lowgrade_2013 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2013 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2013 : chr [1:2101] &quot;670&quot; &quot;238&quot; &quot;66&quot; &quot;582&quot; ... ## $ total_2013 : chr [1:2101] &quot;670&quot; &quot;238&quot; &quot;66&quot; &quot;582&quot; ... ## $ snp_1314 : chr [1:2101] &quot;668&quot; &quot;239&quot; &quot;56&quot; &quot;589&quot; ... ## $ free_elig_1314: chr [1:2101] &quot;346&quot; &quot;102&quot; &quot;12&quot; &quot;347&quot; ... ## $ free_per_1314 : chr [1:2101] &quot;51.80%&quot; &quot;42.68%&quot; &quot;21.43%&quot; &quot;58.91%&quot; ... ## $ red_elig_1314 : chr [1:2101] &quot;44&quot; &quot;19&quot; &quot;4&quot; &quot;54&quot; ... ## $ red_per_1314 : chr [1:2101] &quot;6.59%&quot; &quot;7.95%&quot; &quot;7.14%&quot; &quot;9.17%&quot; ... ## $ totalFRL_1314 : chr [1:2101] &quot;390&quot; &quot;121&quot; &quot;16&quot; &quot;401&quot; ... ## $ totalper_1314 : chr [1:2101] &quot;58.38%&quot; &quot;50.63%&quot; &quot;28.57%&quot; &quot;68.08%&quot; ... ## $ type1415 : chr [1:2101] NA NA NA NA ... ## $ lowgrade_2014 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2014 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2014 : chr [1:2101] &quot;685&quot; &quot;251&quot; &quot;65&quot; &quot;581&quot; ... ## $ total_2014 : chr [1:2101] &quot;685&quot; &quot;251&quot; &quot;65&quot; &quot;581&quot; ... ## $ snp_1415 : chr [1:2101] &quot;672&quot; &quot;239&quot; &quot;61&quot; &quot;586&quot; ... ## $ free_elig_1415: chr [1:2101] &quot;361&quot; &quot;93&quot; &quot;14&quot; &quot;351&quot; ... ## $ free_per_1415 : chr [1:2101] &quot;53.72%&quot; &quot;38.91%&quot; &quot;22.95%&quot; &quot;59.90%&quot; ... ## $ red_elig_1415 : chr [1:2101] &quot;40&quot; &quot;17&quot; &quot;4&quot; &quot;40&quot; ... ## $ red_per_1415 : chr [1:2101] &quot;5.95%&quot; &quot;7.11%&quot; &quot;6.56%&quot; &quot;6.83%&quot; ... ## $ totalFRL_1415 : chr [1:2101] &quot;401&quot; &quot;110&quot; &quot;18&quot; &quot;391&quot; ... ## $ totalper_1415 : chr [1:2101] &quot;59.67%&quot; &quot;46.03%&quot; &quot;29.51%&quot; &quot;66.72%&quot; ... ## $ CEP_1516 : chr [1:2101] &quot;#NULL!&quot; &quot;#NULL!&quot; &quot;#NULL!&quot; &quot;#NULL!&quot; ... ## $ type1516 : chr [1:2101] &quot;SCH-HIGH&quot; &quot;SCH-ELEM&quot; &quot;SCH-COMB&quot; &quot;SCH-HIGH&quot; ... ## $ lowgrade_2015 : chr [1:2101] &quot;9&quot; &quot;PK&quot; &quot;KG&quot; &quot;9&quot; ... ## $ higrade_2015 : chr [1:2101] &quot;12&quot; &quot;5&quot; &quot;12&quot; &quot;12&quot; ... ## $ totalFT_2015 : chr [1:2101] &quot;737&quot; &quot;259&quot; &quot;65&quot; &quot;621&quot; ... ## $ total_2015 : chr [1:2101] &quot;737&quot; &quot;259&quot; &quot;65&quot; &quot;621&quot; ... ## $ snp_1516 : chr [1:2101] &quot;728&quot; &quot;268&quot; &quot;67&quot; &quot;608&quot; ... ## $ free_elig_1516: chr [1:2101] &quot;362&quot; &quot;109&quot; &quot;12&quot; &quot;339&quot; ... ## $ free_per_1516 : chr [1:2101] &quot;49.73%&quot; &quot;40.67%&quot; &quot;17.91%&quot; &quot;55.76%&quot; ... ## [list output truncated] ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. sch_id = col_character(), ## .. div_num = col_double(), ## .. div_name = col_character(), ## .. school_num = col_character(), ## .. school_name = col_character(), ## .. school_name2 = col_character(), ## .. type0809 = col_character(), ## .. lowgrade_2008 = col_character(), ## .. higrade_2008 = col_character(), ## .. totalFT_2008 = col_character(), ## .. total_2008 = col_character(), ## .. snp_0809 = col_character(), ## .. free_elig_0809 = col_character(), ## .. free_per_0809 = col_character(), ## .. red_elig_0809 = col_character(), ## .. red_per_0809 = col_character(), ## .. totalFRL_0809 = col_character(), ## .. totalper_0809 = col_character(), ## .. type0910 = col_character(), ## .. lowgrade_2009 = col_character(), ## .. higrade_2009 = col_character(), ## .. totalFT_2009 = col_character(), ## .. total_2009 = col_character(), ## .. snp_0910 = col_character(), ## .. free_elig_0910 = col_character(), ## .. free_per_0910 = col_character(), ## .. red_elig_0910 = col_character(), ## .. red_per_0910 = col_character(), ## .. totalFRL_09010 = col_character(), ## .. totalper_0910 = col_character(), ## .. type1011 = col_character(), ## .. lowgrade_2010 = col_character(), ## .. higrade_2010 = col_character(), ## .. totalFT_2010 = col_character(), ## .. total_2010 = col_character(), ## .. snp_1011 = col_character(), ## .. free_elig_1011 = col_character(), ## .. free_per_1011 = col_character(), ## .. red_elig_1011 = col_character(), ## .. red_per_1011 = col_character(), ## .. totalFRL_1011 = col_character(), ## .. totalper_1011 = col_character(), ## .. type1112 = col_character(), ## .. lowgrade_2011 = col_character(), ## .. higrade_2011 = col_character(), ## .. totalFT_2011 = col_character(), ## .. total_2011 = col_character(), ## .. snp_1112 = col_character(), ## .. free_elig_1112 = col_character(), ## .. free_per_1112 = col_character(), ## .. red_elig_1112 = col_character(), ## .. red_per_1112 = col_character(), ## .. totalFRL_1112 = col_character(), ## .. totalper_1112 = col_character(), ## .. type1213 = col_character(), ## .. lowgrade_2012 = col_character(), ## .. higrade_2012 = col_character(), ## .. totalFT_2012 = col_character(), ## .. total_2012 = col_character(), ## .. snp_1213 = col_character(), ## .. free_elig_1213 = col_character(), ## .. free_per_1213 = col_character(), ## .. red_elig_1213 = col_character(), ## .. red_per_1213 = col_character(), ## .. totalFRL_1213 = col_character(), ## .. totalper_1213 = col_character(), ## .. type1314 = col_character(), ## .. lowgrade_2013 = col_character(), ## .. higrade_2013 = col_character(), ## .. totalFT_2013 = col_character(), ## .. total_2013 = col_character(), ## .. snp_1314 = col_character(), ## .. free_elig_1314 = col_character(), ## .. free_per_1314 = col_character(), ## .. red_elig_1314 = col_character(), ## .. red_per_1314 = col_character(), ## .. totalFRL_1314 = col_character(), ## .. totalper_1314 = col_character(), ## .. type1415 = col_character(), ## .. lowgrade_2014 = col_character(), ## .. higrade_2014 = col_character(), ## .. totalFT_2014 = col_character(), ## .. total_2014 = col_character(), ## .. snp_1415 = col_character(), ## .. free_elig_1415 = col_character(), ## .. free_per_1415 = col_character(), ## .. red_elig_1415 = col_character(), ## .. red_per_1415 = col_character(), ## .. totalFRL_1415 = col_character(), ## .. totalper_1415 = col_character(), ## .. CEP_1516 = col_character(), ## .. type1516 = col_character(), ## .. lowgrade_2015 = col_character(), ## .. higrade_2015 = col_character(), ## .. totalFT_2015 = col_character(), ## .. total_2015 = col_character(), ## .. snp_1516 = col_character(), ## .. free_elig_1516 = col_character(), ## .. free_per_1516 = col_character(), ## .. red_elig_1516 = col_character(), ## .. red_Per_1516 = col_character(), ## .. totalFRL_1516 = col_character(), ## .. totalper_1516 = col_character(), ## .. CEP_1617 = col_character(), ## .. type1617 = col_character(), ## .. lowgrade_2016 = col_character(), ## .. higrade_2016 = col_character(), ## .. totalFT_2016 = col_character(), ## .. total_2016 = col_character(), ## .. snp_2016 = col_character(), ## .. free_elig_1617 = col_character(), ## .. free_per_1617 = col_character(), ## .. red_elig_1617 = col_character(), ## .. red_per_1617 = col_character(), ## .. totalFRL_1617 = col_character(), ## .. totalper_1617 = col_character(), ## .. CEP_1718 = col_character(), ## .. type1718 = col_character(), ## .. lowgrade_2017 = col_character(), ## .. higrade_2017 = col_character(), ## .. totalFT_2017 = col_character(), ## .. total_2017 = col_character(), ## .. snp_1718 = col_character(), ## .. free_elig_1718 = col_character(), ## .. free_per_1718 = col_character(), ## .. red_elig_1718 = col_character(), ## .. red_per_1718 = col_character(), ## .. totalFRL_1718 = col_character(), ## .. totalper_1718 = col_character(), ## .. stable = col_character(), ## .. new = col_character(), ## .. closed = col_character(), ## .. close_yr = col_character(), ## .. reuseid = col_character(), ## .. gradechg = col_character(), ## .. gradechg_yr = col_character(), ## .. grchgyr_2 = col_character() ## .. ) Or just check the structure of one specific variable. str(mydata$total_2017) ## chr [1:2101] &quot;742&quot; &quot;236&quot; &quot;60&quot; &quot;624&quot; &quot;286&quot; &quot;485&quot; &quot;583&quot; &quot;550&quot; &quot;600&quot; &quot;514&quot; &quot;519&quot; &quot;468&quot; &quot;351&quot; &quot;264&quot; &quot;675&quot; &quot;565&quot; ... NOTE: When you have a lot of variables, running this str() function is not a great idea - the output is a little too cumbersome 3.5 Mutating Variables Note that almost all of the data reads in as a character data type which are just strings, This can create issues. We know that many of the columns are actually storing numbers or numeric values as R refers to them. We need to fix this. Lets tell R that these columns (at least the two we are going to use) are numeric. We are going to see two interchangeable ways to do this. First, we use the $ operator which lets me specify a specific column within my data frame in combination with the as.numeric() function mydata$total_2017&lt;-as.numeric(mydata$total_2017) mydata$totalFRL_1718&lt;-as.numeric(mydata$totalFRL_1718) Some columns have a percent symbol, which you will need to remove before coercing to numeric data type mydata &lt;- mydata %&gt;% mutate(totalper_0809 = str_remove(totalper_0809, &quot;%&quot;)) Then we can change the column from character to numeric mydata$totalper_0809 &lt;- as.numeric(mydata$totalper_0809) ## Warning: NAs introduced by coercion Check to make sure it converted the column type correctly using str(). str(mydata$totalper_0809) ## num [1:2101] 56.1 40.1 47.5 55.5 33.4 ... Second, alternatively, we can do this for a whole set of variables at once. We just need to specify a matching criteria. newdf &lt;- mydata %&gt;% mutate_at(vars(starts_with(&quot;total&quot;)), as.numeric) ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion newdf &lt;- newdf %&gt;% mutate_at(vars(starts_with(&quot;totalFRL&quot;)), as.numeric) Check whether the old and new variables are stored differently (old as a character, new as a numeric variable) str(mydata$total_2008) ## chr [1:2101] &quot;731&quot; &quot;263&quot; &quot;80&quot; &quot;638&quot; &quot;333&quot; &quot;536&quot; &quot;610&quot; &quot;490&quot; &quot;585&quot; &quot;450&quot; &quot;477&quot; &quot;502&quot; &quot;359&quot; &quot;187&quot; &quot;439&quot; &quot;491&quot; ... str(newdf$total_2008) ## num [1:2101] 731 263 80 638 333 536 610 490 585 450 ... 3.6 Filtering and Selecting A basic operation we do a lot is to filter the data so that we are working with a subset of all that we have. We can do this with the filter() function, part of the dplyr package (in the tidyverse collection of packages). Lets say we want to look at the schools with div_num values less than 50. newdf %&gt;% filter(div_num &lt; 50) ## # A tibble: 800 x 137 ## sch_id div_num div_name school_num school_name school_name2 type0809 lowgrade_2008 higrade_2008 totalFT_2008 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 001-0~ 1 Accomac~ &quot;0070\\xa0&quot; NANDUA HIGH &lt;NA&gt; SCH-HIGH 9 12 731 ## 2 001-0~ 1 Accomac~ &quot;0080\\xa0&quot; CHINCOTEAG~ &lt;NA&gt; SCH-ELEM PK 5 263 ## 3 001-0~ 1 Accomac~ &quot;0530\\xa0&quot; TANGIER CO~ &lt;NA&gt; SCH-COMB KG 12 80 ## 4 001-0~ 1 Accomac~ &quot;0540\\xa0&quot; ARCADIA HI~ &lt;NA&gt; SCH-HIGH 9 12 638 ## 5 001-0~ 1 Accomac~ &quot;0580\\xa0&quot; CHINCOTEAG~ &lt;NA&gt; SCH-COMB 6 12 333 ## 6 001-0~ 1 Accomac~ &quot;0590\\xa0&quot; PUNGOTEAGU~ &lt;NA&gt; SCH-ELEM PK 5 536 ## 7 001-0~ 1 Accomac~ &quot;0600\\xa0&quot; KEGOTANK E~ &lt;NA&gt; SCH-ELEM PK 5 610 ## 8 001-0~ 1 Accomac~ &quot;0701\\xa0&quot; ACCAWMACKE~ &lt;NA&gt; SCH-ELEM PK 5 490 ## 9 001-0~ 1 Accomac~ &quot;0702\\xa0&quot; METOMPKIN ~ &lt;NA&gt; SCH-ELEM PK 5 585 ## 10 001-0~ 1 Accomac~ &quot;0703\\xa0&quot; NANDUA MID~ &lt;NA&gt; SCH-MID 6 8 450 ## # ... with 790 more rows, and 127 more variables: total_2008 &lt;dbl&gt;, snp_0809 &lt;chr&gt;, free_elig_0809 &lt;chr&gt;, ## # free_per_0809 &lt;chr&gt;, red_elig_0809 &lt;chr&gt;, red_per_0809 &lt;chr&gt;, totalFRL_0809 &lt;dbl&gt;, totalper_0809 &lt;dbl&gt;, ## # type0910 &lt;chr&gt;, lowgrade_2009 &lt;chr&gt;, higrade_2009 &lt;chr&gt;, totalFT_2009 &lt;dbl&gt;, total_2009 &lt;dbl&gt;, snp_0910 &lt;chr&gt;, ## # free_elig_0910 &lt;chr&gt;, free_per_0910 &lt;chr&gt;, red_elig_0910 &lt;chr&gt;, red_per_0910 &lt;chr&gt;, totalFRL_09010 &lt;dbl&gt;, ## # totalper_0910 &lt;dbl&gt;, type1011 &lt;chr&gt;, lowgrade_2010 &lt;chr&gt;, higrade_2010 &lt;chr&gt;, totalFT_2010 &lt;dbl&gt;, ## # total_2010 &lt;dbl&gt;, snp_1011 &lt;chr&gt;, free_elig_1011 &lt;chr&gt;, free_per_1011 &lt;chr&gt;, red_elig_1011 &lt;chr&gt;, ## # red_per_1011 &lt;chr&gt;, totalFRL_1011 &lt;dbl&gt;, totalper_1011 &lt;dbl&gt;, type1112 &lt;chr&gt;, lowgrade_2011 &lt;chr&gt;, ## # higrade_2011 &lt;chr&gt;, totalFT_2011 &lt;dbl&gt;, total_2011 &lt;dbl&gt;, snp_1112 &lt;chr&gt;, free_elig_1112 &lt;chr&gt;, ## # free_per_1112 &lt;chr&gt;, red_elig_1112 &lt;chr&gt;, red_per_1112 &lt;chr&gt;, totalFRL_1112 &lt;dbl&gt;, totalper_1112 &lt;dbl&gt;, ## # type1213 &lt;chr&gt;, lowgrade_2012 &lt;chr&gt;, higrade_2012 &lt;chr&gt;, totalFT_2012 &lt;dbl&gt;, total_2012 &lt;dbl&gt;, snp_1213 &lt;chr&gt;, ## # free_elig_1213 &lt;chr&gt;, free_per_1213 &lt;chr&gt;, red_elig_1213 &lt;chr&gt;, red_per_1213 &lt;chr&gt;, totalFRL_1213 &lt;dbl&gt;, ## # totalper_1213 &lt;dbl&gt;, type1314 &lt;chr&gt;, lowgrade_2013 &lt;chr&gt;, higrade_2013 &lt;chr&gt;, totalFT_2013 &lt;dbl&gt;, ## # total_2013 &lt;dbl&gt;, snp_1314 &lt;chr&gt;, free_elig_1314 &lt;chr&gt;, free_per_1314 &lt;chr&gt;, red_elig_1314 &lt;chr&gt;, ## # red_per_1314 &lt;chr&gt;, totalFRL_1314 &lt;dbl&gt;, totalper_1314 &lt;dbl&gt;, type1415 &lt;chr&gt;, lowgrade_2014 &lt;chr&gt;, ## # higrade_2014 &lt;chr&gt;, totalFT_2014 &lt;dbl&gt;, total_2014 &lt;dbl&gt;, snp_1415 &lt;chr&gt;, free_elig_1415 &lt;chr&gt;, ## # free_per_1415 &lt;chr&gt;, red_elig_1415 &lt;chr&gt;, red_per_1415 &lt;chr&gt;, totalFRL_1415 &lt;dbl&gt;, totalper_1415 &lt;dbl&gt;, ## # CEP_1516 &lt;chr&gt;, type1516 &lt;chr&gt;, lowgrade_2015 &lt;chr&gt;, higrade_2015 &lt;chr&gt;, totalFT_2015 &lt;dbl&gt;, total_2015 &lt;dbl&gt;, ## # snp_1516 &lt;chr&gt;, free_elig_1516 &lt;chr&gt;, free_per_1516 &lt;chr&gt;, red_elig_1516 &lt;chr&gt;, red_Per_1516 &lt;chr&gt;, ## # totalFRL_1516 &lt;dbl&gt;, totalper_1516 &lt;dbl&gt;, CEP_1617 &lt;chr&gt;, type1617 &lt;chr&gt;, lowgrade_2016 &lt;chr&gt;, ## # higrade_2016 &lt;chr&gt;, totalFT_2016 &lt;dbl&gt;, total_2016 &lt;dbl&gt;, snp_2016 &lt;chr&gt;, ... Or, if we want to look at schools where the highest grade in 2008 was grade five, we can try: newdf %&gt;% filter(higrade_2008 == &quot;5&quot;) # this returns a subsetted dataframe with 878 rows ## # A tibble: 878 x 137 ## sch_id div_num div_name school_num school_name school_name2 type0809 lowgrade_2008 higrade_2008 totalFT_2008 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 001-0~ 1 Accomac~ &quot;0080\\xa0&quot; CHINCOTEAG~ &lt;NA&gt; SCH-ELEM PK 5 263 ## 2 001-0~ 1 Accomac~ &quot;0590\\xa0&quot; PUNGOTEAGU~ &lt;NA&gt; SCH-ELEM PK 5 536 ## 3 001-0~ 1 Accomac~ &quot;0600\\xa0&quot; KEGOTANK E~ &lt;NA&gt; SCH-ELEM PK 5 610 ## 4 001-0~ 1 Accomac~ &quot;0701\\xa0&quot; ACCAWMACKE~ &lt;NA&gt; SCH-ELEM PK 5 490 ## 5 001-0~ 1 Accomac~ &quot;0702\\xa0&quot; METOMPKIN ~ &lt;NA&gt; SCH-ELEM PK 5 585 ## 6 002-0~ 2 Albemar~ &quot;0010\\xa0&quot; HOLLYMEAD ~ &lt;NA&gt; SCH-ELEM PK 5 502 ## 7 002-0~ 2 Albemar~ &quot;0030\\xa0&quot; SCOTTSVILL~ &lt;NA&gt; SCH-ELEM PK 5 187 ## 8 002-0~ 2 Albemar~ &quot;0040\\xa0&quot; MARY CARR ~ &lt;NA&gt; SCH-ELEM PK 5 439 ## 9 002-0~ 2 Albemar~ &quot;0100\\xa0&quot; BROADUS WO~ &lt;NA&gt; SCH-ELEM PK 5 289 ## 10 002-0~ 2 Albemar~ &quot;0150\\xa0&quot; PAUL H CAL~ &lt;NA&gt; SCH-ELEM PK 5 575 ## # ... with 868 more rows, and 127 more variables: total_2008 &lt;dbl&gt;, snp_0809 &lt;chr&gt;, free_elig_0809 &lt;chr&gt;, ## # free_per_0809 &lt;chr&gt;, red_elig_0809 &lt;chr&gt;, red_per_0809 &lt;chr&gt;, totalFRL_0809 &lt;dbl&gt;, totalper_0809 &lt;dbl&gt;, ## # type0910 &lt;chr&gt;, lowgrade_2009 &lt;chr&gt;, higrade_2009 &lt;chr&gt;, totalFT_2009 &lt;dbl&gt;, total_2009 &lt;dbl&gt;, snp_0910 &lt;chr&gt;, ## # free_elig_0910 &lt;chr&gt;, free_per_0910 &lt;chr&gt;, red_elig_0910 &lt;chr&gt;, red_per_0910 &lt;chr&gt;, totalFRL_09010 &lt;dbl&gt;, ## # totalper_0910 &lt;dbl&gt;, type1011 &lt;chr&gt;, lowgrade_2010 &lt;chr&gt;, higrade_2010 &lt;chr&gt;, totalFT_2010 &lt;dbl&gt;, ## # total_2010 &lt;dbl&gt;, snp_1011 &lt;chr&gt;, free_elig_1011 &lt;chr&gt;, free_per_1011 &lt;chr&gt;, red_elig_1011 &lt;chr&gt;, ## # red_per_1011 &lt;chr&gt;, totalFRL_1011 &lt;dbl&gt;, totalper_1011 &lt;dbl&gt;, type1112 &lt;chr&gt;, lowgrade_2011 &lt;chr&gt;, ## # higrade_2011 &lt;chr&gt;, totalFT_2011 &lt;dbl&gt;, total_2011 &lt;dbl&gt;, snp_1112 &lt;chr&gt;, free_elig_1112 &lt;chr&gt;, ## # free_per_1112 &lt;chr&gt;, red_elig_1112 &lt;chr&gt;, red_per_1112 &lt;chr&gt;, totalFRL_1112 &lt;dbl&gt;, totalper_1112 &lt;dbl&gt;, ## # type1213 &lt;chr&gt;, lowgrade_2012 &lt;chr&gt;, higrade_2012 &lt;chr&gt;, totalFT_2012 &lt;dbl&gt;, total_2012 &lt;dbl&gt;, snp_1213 &lt;chr&gt;, ## # free_elig_1213 &lt;chr&gt;, free_per_1213 &lt;chr&gt;, red_elig_1213 &lt;chr&gt;, red_per_1213 &lt;chr&gt;, totalFRL_1213 &lt;dbl&gt;, ## # totalper_1213 &lt;dbl&gt;, type1314 &lt;chr&gt;, lowgrade_2013 &lt;chr&gt;, higrade_2013 &lt;chr&gt;, totalFT_2013 &lt;dbl&gt;, ## # total_2013 &lt;dbl&gt;, snp_1314 &lt;chr&gt;, free_elig_1314 &lt;chr&gt;, free_per_1314 &lt;chr&gt;, red_elig_1314 &lt;chr&gt;, ## # red_per_1314 &lt;chr&gt;, totalFRL_1314 &lt;dbl&gt;, totalper_1314 &lt;dbl&gt;, type1415 &lt;chr&gt;, lowgrade_2014 &lt;chr&gt;, ## # higrade_2014 &lt;chr&gt;, totalFT_2014 &lt;dbl&gt;, total_2014 &lt;dbl&gt;, snp_1415 &lt;chr&gt;, free_elig_1415 &lt;chr&gt;, ## # free_per_1415 &lt;chr&gt;, red_elig_1415 &lt;chr&gt;, red_per_1415 &lt;chr&gt;, totalFRL_1415 &lt;dbl&gt;, totalper_1415 &lt;dbl&gt;, ## # CEP_1516 &lt;chr&gt;, type1516 &lt;chr&gt;, lowgrade_2015 &lt;chr&gt;, higrade_2015 &lt;chr&gt;, totalFT_2015 &lt;dbl&gt;, total_2015 &lt;dbl&gt;, ## # snp_1516 &lt;chr&gt;, free_elig_1516 &lt;chr&gt;, free_per_1516 &lt;chr&gt;, red_elig_1516 &lt;chr&gt;, red_Per_1516 &lt;chr&gt;, ## # totalFRL_1516 &lt;dbl&gt;, totalper_1516 &lt;dbl&gt;, CEP_1617 &lt;chr&gt;, type1617 &lt;chr&gt;, lowgrade_2016 &lt;chr&gt;, ## # higrade_2016 &lt;chr&gt;, totalFT_2016 &lt;dbl&gt;, total_2016 &lt;dbl&gt;, snp_2016 &lt;chr&gt;, ... Note that we had to set it equal to the character value 5 rather than the numeric value 5. Why? If we wanted to filter on numeric values instead, we would want to do something like this: newdf %&gt;% mutate(higrade_2008 = as.numeric(higrade_2008)) %&gt;% filter(higrade_2008 == 5) # again, this returns a subsetted dataframe with 878 rows ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## # A tibble: 878 x 137 ## sch_id div_num div_name school_num school_name school_name2 type0809 lowgrade_2008 higrade_2008 totalFT_2008 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 001-0~ 1 Accomac~ &quot;0080\\xa0&quot; CHINCOTEAG~ &lt;NA&gt; SCH-ELEM PK 5 263 ## 2 001-0~ 1 Accomac~ &quot;0590\\xa0&quot; PUNGOTEAGU~ &lt;NA&gt; SCH-ELEM PK 5 536 ## 3 001-0~ 1 Accomac~ &quot;0600\\xa0&quot; KEGOTANK E~ &lt;NA&gt; SCH-ELEM PK 5 610 ## 4 001-0~ 1 Accomac~ &quot;0701\\xa0&quot; ACCAWMACKE~ &lt;NA&gt; SCH-ELEM PK 5 490 ## 5 001-0~ 1 Accomac~ &quot;0702\\xa0&quot; METOMPKIN ~ &lt;NA&gt; SCH-ELEM PK 5 585 ## 6 002-0~ 2 Albemar~ &quot;0010\\xa0&quot; HOLLYMEAD ~ &lt;NA&gt; SCH-ELEM PK 5 502 ## 7 002-0~ 2 Albemar~ &quot;0030\\xa0&quot; SCOTTSVILL~ &lt;NA&gt; SCH-ELEM PK 5 187 ## 8 002-0~ 2 Albemar~ &quot;0040\\xa0&quot; MARY CARR ~ &lt;NA&gt; SCH-ELEM PK 5 439 ## 9 002-0~ 2 Albemar~ &quot;0100\\xa0&quot; BROADUS WO~ &lt;NA&gt; SCH-ELEM PK 5 289 ## 10 002-0~ 2 Albemar~ &quot;0150\\xa0&quot; PAUL H CAL~ &lt;NA&gt; SCH-ELEM PK 5 575 ## # ... with 868 more rows, and 127 more variables: total_2008 &lt;dbl&gt;, snp_0809 &lt;chr&gt;, free_elig_0809 &lt;chr&gt;, ## # free_per_0809 &lt;chr&gt;, red_elig_0809 &lt;chr&gt;, red_per_0809 &lt;chr&gt;, totalFRL_0809 &lt;dbl&gt;, totalper_0809 &lt;dbl&gt;, ## # type0910 &lt;chr&gt;, lowgrade_2009 &lt;chr&gt;, higrade_2009 &lt;chr&gt;, totalFT_2009 &lt;dbl&gt;, total_2009 &lt;dbl&gt;, snp_0910 &lt;chr&gt;, ## # free_elig_0910 &lt;chr&gt;, free_per_0910 &lt;chr&gt;, red_elig_0910 &lt;chr&gt;, red_per_0910 &lt;chr&gt;, totalFRL_09010 &lt;dbl&gt;, ## # totalper_0910 &lt;dbl&gt;, type1011 &lt;chr&gt;, lowgrade_2010 &lt;chr&gt;, higrade_2010 &lt;chr&gt;, totalFT_2010 &lt;dbl&gt;, ## # total_2010 &lt;dbl&gt;, snp_1011 &lt;chr&gt;, free_elig_1011 &lt;chr&gt;, free_per_1011 &lt;chr&gt;, red_elig_1011 &lt;chr&gt;, ## # red_per_1011 &lt;chr&gt;, totalFRL_1011 &lt;dbl&gt;, totalper_1011 &lt;dbl&gt;, type1112 &lt;chr&gt;, lowgrade_2011 &lt;chr&gt;, ## # higrade_2011 &lt;chr&gt;, totalFT_2011 &lt;dbl&gt;, total_2011 &lt;dbl&gt;, snp_1112 &lt;chr&gt;, free_elig_1112 &lt;chr&gt;, ## # free_per_1112 &lt;chr&gt;, red_elig_1112 &lt;chr&gt;, red_per_1112 &lt;chr&gt;, totalFRL_1112 &lt;dbl&gt;, totalper_1112 &lt;dbl&gt;, ## # type1213 &lt;chr&gt;, lowgrade_2012 &lt;chr&gt;, higrade_2012 &lt;chr&gt;, totalFT_2012 &lt;dbl&gt;, total_2012 &lt;dbl&gt;, snp_1213 &lt;chr&gt;, ## # free_elig_1213 &lt;chr&gt;, free_per_1213 &lt;chr&gt;, red_elig_1213 &lt;chr&gt;, red_per_1213 &lt;chr&gt;, totalFRL_1213 &lt;dbl&gt;, ## # totalper_1213 &lt;dbl&gt;, type1314 &lt;chr&gt;, lowgrade_2013 &lt;chr&gt;, higrade_2013 &lt;chr&gt;, totalFT_2013 &lt;dbl&gt;, ## # total_2013 &lt;dbl&gt;, snp_1314 &lt;chr&gt;, free_elig_1314 &lt;chr&gt;, free_per_1314 &lt;chr&gt;, red_elig_1314 &lt;chr&gt;, ## # red_per_1314 &lt;chr&gt;, totalFRL_1314 &lt;dbl&gt;, totalper_1314 &lt;dbl&gt;, type1415 &lt;chr&gt;, lowgrade_2014 &lt;chr&gt;, ## # higrade_2014 &lt;chr&gt;, totalFT_2014 &lt;dbl&gt;, total_2014 &lt;dbl&gt;, snp_1415 &lt;chr&gt;, free_elig_1415 &lt;chr&gt;, ## # free_per_1415 &lt;chr&gt;, red_elig_1415 &lt;chr&gt;, red_per_1415 &lt;chr&gt;, totalFRL_1415 &lt;dbl&gt;, totalper_1415 &lt;dbl&gt;, ## # CEP_1516 &lt;chr&gt;, type1516 &lt;chr&gt;, lowgrade_2015 &lt;chr&gt;, higrade_2015 &lt;chr&gt;, totalFT_2015 &lt;dbl&gt;, total_2015 &lt;dbl&gt;, ## # snp_1516 &lt;chr&gt;, free_elig_1516 &lt;chr&gt;, free_per_1516 &lt;chr&gt;, red_elig_1516 &lt;chr&gt;, red_Per_1516 &lt;chr&gt;, ## # totalFRL_1516 &lt;dbl&gt;, totalper_1516 &lt;dbl&gt;, CEP_1617 &lt;chr&gt;, type1617 &lt;chr&gt;, lowgrade_2016 &lt;chr&gt;, ## # higrade_2016 &lt;chr&gt;, totalFT_2016 &lt;dbl&gt;, total_2016 &lt;dbl&gt;, snp_2016 &lt;chr&gt;, ... 3.7 Grouping and Summarizing Lets shift gears to a different combination of operations Lets go ahead and try using tidyverse to narrow to what we want. Imagine we want to see the county level aggregate numbers for FRL in the 2017-2018 school year. We will start out with our entire data frame and then use pipes (the %&gt;% operator) to work from there. The final result will be stored in our new data frame that we are creating, called county_level_aggregate. First, select will pick columns Next, group_by and summarize work together to get us our aggregate totals. county_level_aggregate &lt;- newdf %&gt;% select(div_name, total_2017, totalFRL_1718) %&gt;% group_by(div_name) %&gt;% summarize(totalstudents = sum(total_2017), totalFRL = sum(totalFRL_1718)) ## `summarise()` ungrouping output (override with `.groups` argument) Now, we can compute percentages if we like and we can specify a new column by referring to. One that doesnt exist yet but will after we run this code. We will do this two interchangeable ways. First, the old school way: county_level_aggregate$percent_FRL &lt;- county_level_aggregate$totalFRL/county_level_aggregate$totalstudents*100 Second, the tidyverse way: county_level_aggregate &lt;- county_level_aggregate %&gt;% mutate(percent_frl = totalFRL / totalstudents * 100) Just for fun, lets see how this could have been incorporated into our summarize call county_level_percents &lt;- newdf %&gt;% select(div_name, total_2017, totalFRL_1718) %&gt;% group_by(div_name) %&gt;% summarize(percentFRL=sum(totalFRL_1718)/sum(total_2017) * 100) ## `summarise()` ungrouping output (override with `.groups` argument) Something is going to look weird with this plot newdf %&gt;% ggplot(aes(totalFRL_0809, totalFT_2008)) + geom_point() + labs(title = &quot;FRL 2008&quot;, x = &quot;totalFRL_0809&quot;) ## Warning: Removed 236 rows containing missing values (geom_point). Lets see if we can fix it newdf %&gt;% filter(!is.na(totalFRL_0809)) %&gt;% ggplot(aes(totalFRL_0809, totalFT_2008)) + geom_point() + labs(title = &quot;FRL 2008&quot;, x = &quot;totalFRL_0809&quot;) + xlim(0, 1000) + ylim(0, 1000) ## Warning: Removed 324 rows containing missing values (geom_point). "],["week-4-assumptions-and-correlations.html", "Chapter4 Week 4: Assumptions and Correlations 4.1 Assumptions 4.2 Correlation 4.3 Another worked example for cleaning and prelim analysis", " Chapter4 Week 4: Assumptions and Correlations This week we will be discussing Chapters 5 and 6 from DSUR. These notes will pull out some of the important pieces from each chapter. 4.1 Assumptions These assumptions 4.1.1 Normally distributed data This assumption is about the normality of the sampling distribution. The big idea here is that we tend to operate under the belief that if our sampled data are normally distributed then the underlying sampling distribution is also normally distributed. Also, keep in mind that this becomes less of a concern as our sample size increases (thank, Central Limit Theorem!). There are several tests for normality that we will discuss, which include either (a) calculations or (b) visual examination. We will discuss both. 4.1.1.1 Visual check of normality 4.1.2 Homogeneity of variance 4.1.3 Interval data 4.1.4 Independence 4.2 Correlation 4.2.1 Covariance First, start with the observation that variance is calculated with: \\(Variance(s^2) = \\frac{\\sum(x_i - \\overline{x})^2}{N - 1} = \\frac{\\sum(x_i - \\overline{x})(x_i - \\overline{x})}{N - 1}\\) But now lets say that we want to know how, for each observation we have, how does the value of \\(x\\) vary with the value of \\(y\\) on average. For example, when the value of \\(x\\) increases, does the value of \\(y\\) also increase? This could happen when \\(x\\) represents the number of hours of sleep you get each night and \\(y\\) is your average grade on an exam you take the next day. The opposite could arise when \\(x\\) increases but we expect \\(y\\) to go down. An example of this might be when \\(x\\) is the number of hilarious jokes that a teacher tells in class and \\(y\\) is the number of students who fall asleep in class. As the number of jokes increases, we might expect/hope that it keeps students attention and keeps them from dozing. This generally process of considering how one variable changes when another variable changes is where the notion of covariance comes in. In practice, what we really want to know is: when \\(x_i\\) is above its average value in a sample (\\(\\overline{x}\\)), how does \\(y_i\\) change? Does it also tend to be above the sample average for \\(y\\) (\\(\\overline{y}\\))? This is expressed in the general formular for covariance: \\(cov(x,y) = \\frac{\\sum(x_i - \\overline{x})(y_i - \\overline{y})}{N - 1}\\) While covariance can be a helpful value to work with in many settings, for most of what we do in this class, we will be using correlation coefficients instead of covariance. This is because covariance is an unnormalized value, which can make comparisons across different ranges of values difficult. 4.2.2 Correlation coefficient In order to standardize the covariance to a value the is easier to work with across ranges of values, we use the correlatoin coefficient. There are several version of this, depending on the type of data you are working with. The most basic version is the Pearson correlation coefficient. It is calculated by dividing the covariance by the standard deviations of your two variables of interest: \\(r = \\frac{cov_{xy}}{s_xs_y} = \\frac{\\sum(x_i - \\overline{x})(y_i - \\overline{y})}{(N-1)s_xs_y}\\) This is a bivariate correlation coefficient because it is looking at the correlation between two variables. There are also partial correlation coefficients, which look at the correlation between two variables whil controlling for other variables. We can calculate the correlatoin between two variables using the cor() or cor.test() functions, which are part of base R. 4.3 Another worked example for cleaning and prelim analysis This script takes an incomplete subset of senior data from a .csv file, cleans it, computes factor scores, and prepares it for analysis. If you have not already done so, make sure that you have run library(tidyverse) and library(psych) since we will be using functions from both of those packages. 4.3.1 Loading in data First, as usual, load in your data. We will use the file seniorsurvey.csv for this demo. file_path &lt;- \"YOUR PATH HERE\" setwd(file_path) use this command to change the working directory to the folder where you have your file list.files() run this to make sure that your file is in your current working directory seniorSurvey_df &lt;- read_csv(&quot;seniorsurvey.csv&quot;) # replace text in the parentheses with your file name ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. 4.3.2 Data prep and cleaning After loading, it is always nice to just see how things loaded in. Functions like str() and describe() from the psych package are nice for this. For example, if we use describe(), we can see the following (we deleted some variables): psych::describe(seniorSurvey_df) ## vars n mean sd median trimmed mad min max range skew ## What is your PRIMARY MAJOR? 1 1849 31.79 20.43 29 31.25 28.17 1 70 69 0.18 ## Internship, field experience, co-op, or practicum 2 1121 1.00 0.00 1 1.00 0.00 1 1 0 NaN ## ParticipateServiceL 3 489 1.00 0.00 1 1.00 0.00 1 1 0 NaN ## ParticipateCService 4 1296 1.00 0.00 1 1.00 0.00 1 1 0 NaN ## ParticipateStudyAbroadSemester 5 142 1.00 0.00 1 1.00 0.00 1 1 0 NaN ## SJ1 6 1733 2.32 1.03 2 2.25 1.48 1 5 4 0.46 ## SJ2 7 1732 2.08 0.96 2 1.98 1.48 1 5 4 0.64 ## SJ3 8 1731 2.77 0.88 3 2.79 1.48 1 5 4 -0.03 ## SJ4 9 1726 2.27 1.01 2 2.20 1.48 1 5 4 0.52 ## SJ5 10 1728 3.27 0.92 3 3.27 1.48 1 5 4 -0.19 ## SJ6 11 1719 3.50 0.83 4 3.54 1.48 1 5 4 -0.40 ## SJ7 12 1719 4.01 0.79 4 4.07 0.00 1 5 4 -0.75 ## SJ8 13 1719 4.15 0.83 4 4.23 1.48 1 5 4 -0.86 ## DA1 14 1719 2.23 0.93 2 2.15 1.48 1 5 4 0.57 ## DA2 15 1719 2.86 0.95 3 2.89 1.48 1 5 4 -0.09 ## DA3 16 1720 1.97 0.81 2 1.90 0.00 1 5 4 0.70 ## DA4 17 1721 4.20 0.72 4 4.27 1.48 1 5 4 -0.66 ## DA5 18 1721 4.06 0.81 4 4.11 1.48 1 5 4 -0.59 ## LocalRole 19 1453 3.49 0.94 4 3.51 1.48 1 5 4 -0.44 ## LocalFinance 20 1453 3.22 0.90 3 3.23 1.48 1 5 4 -0.19 ## LocalTime 21 1453 3.58 0.89 4 3.63 1.48 1 5 4 -0.59 ## GlobalRole 22 1446 3.57 0.99 4 3.62 1.48 1 5 4 -0.47 ## GlobalFinance 23 1449 3.19 1.00 3 3.18 1.48 1 5 4 -0.11 ## GlobalTime 24 1449 3.42 0.98 3 3.44 1.48 1 5 4 -0.35 ## Your gender? 25 1678 1.49 0.50 1 1.49 0.00 1 2 1 0.03 ## kurtosis se ## What is your PRIMARY MAJOR? -1.33 0.48 ## Internship, field experience, co-op, or practicum NaN 0.00 ## ParticipateServiceL NaN 0.00 ## ParticipateCService NaN 0.00 ## ParticipateStudyAbroadSemester NaN 0.00 ## SJ1 -0.51 0.02 ## SJ2 -0.16 0.02 ## SJ3 -0.14 0.02 ## SJ4 -0.40 0.02 ## SJ5 -0.03 0.02 ## SJ6 0.15 0.02 ## SJ7 1.05 0.02 ## SJ8 0.69 0.02 ## DA1 -0.11 0.02 ## DA2 -0.41 0.02 ## DA3 0.31 0.02 ## DA4 0.54 0.02 ## DA5 0.16 0.02 ## LocalRole -0.33 0.02 ## LocalFinance -0.27 0.02 ## LocalTime 0.22 0.02 ## GlobalRole -0.30 0.03 ## GlobalFinance -0.33 0.03 ## GlobalTime -0.17 0.03 ## Your gender? -2.00 0.01 Upon examining this, we can notice a few things: Primary Major variable is all messed up. We wont fix it here, but basically there is a numeric code needed (e.g., 13 = underwater basket weaving) Columns 3 and 5 have lots of missing values (note the small Ns)  this means that this was asked via checkbox so (1) is true and missing is not missing but False SJ1-8 and DA1-5 all look essentially ok  about the same N (some survey fatigue or skips) but all values in range (1-5) Now, we know that SJ and DA are scales from the literature and we want to compute scale scores for those. Typically for attitude scales like these we just report means across the items. So, we will use the psych package to use a built in function to help us with this. If you have not used psych yet, be sure it is installed using the command install.packages(psych)  you need only do this once and then in subsequent uses you only need library(psych) to tell R to look in that package for the functions you will be using. library(psych) Subset out only the SJ and DA items in their own dataframe and then use tools in the psych package to compute scale means The first method to do this - use numbering of the columns: seniorSurveyScales_df &lt;- seniorSurvey_df[6:18] A second method to do this - use select() from dplyr seniorSurveyScales_df &lt;- seniorSurvey_df %&gt;% select(SJ1:DA5) Use the make.keys() function from psych package to key-in how the scales are built (mapping items to scales, use - for reverse scored items) my_keys &lt;- make.keys(seniorSurveyScales_df, list(SJCa=c(-1,-2,-3,-4),SJCh=c(5,6,7),DA=c(-9,-10,-11,12,13))) Use scoreItems function to score each respondent on the three scales of interest SJCa, SJCh, and DA  the default here in scoreItems is to takes the mean of the items (not additive though that is sometimes used) and also, it imputes missing values instead of dropping cases the scoreItems function calculates many things. At this stage, all we really want are the scores, so we include a line to only extract that info. my_scales &lt;- scoreItems(my_keys, seniorSurveyScales_df) my_scores &lt;- my_scales$scores Now, if you view the first few rows of the my.scores vector using the header  head() command  it looks like we expect: head(my_scores) ## SJCa SJCh DA ## [1,] 2.75 3.000000 3.2 ## [2,] 3.75 3.333333 4.2 ## [3,] 3.00 3.000000 3.0 ## [4,] 2.25 4.333333 3.6 ## [5,] 3.00 3.333333 3.4 ## [6,] 4.50 4.333333 3.4 Now, lets build a clean dataframe to prep for analysis - by clean in this case I mean that we have replaced item scores from the scales with their means and also that we have fixed the NAs that dont belong (for participation variables, in this dataset, the NAs should be 0s) my_df &lt;- data.frame(seniorSurvey_df[1:5],my_scores, seniorSurvey_df[19:25]) This is an old school method to replace NAs in specific columns my_df$ParticipateServiceL[is.na(my_df$ParticipateServiceL)] &lt;- 0 my_df$ParticipateCService[is.na(my_df$ParticipateCService)] &lt;- 0 my_df$ParticipateStudyAbroadSemester[is.na(my_df$ParticipateStudyAbroadSemester)] &lt;- 0 my_df\\(ParticipateInternCoop...[is.na(mydf\\)ParticipateInternCoop)] &lt;- 0 - this variable read in cumbersomely named and I dont care about it right now so Ill skip Here is An alternative method to replace NAs in specific columns: my_df &lt;- my_df %&gt;% replace_na(list(ParticipateCService = 0, ParticipateStudyAbroadSemester = 0, ParticipateServiceL = 0)) 4.3.3 Preliminary analysis At this point, we are ready for some analysis Lets investigate correlations. What seems most obvious would just be to run cor() but, as we found out in class, this can cause us to run full speed ahead without considering assumptions my_correlations &lt;- my_df %&gt;% select(SJCa,SJCh,DA) %&gt;% cor() print(my_correlations) ## SJCa SJCh DA ## SJCa 1.0000000 0.2590211 0.3342276 ## SJCh 0.2590211 1.0000000 0.2310703 ## DA 0.3342276 0.2310703 1.0000000 Ok, so, it is important that we note that this ran correlations but R doesnt know that this was sample data and therefore that we are interested instatistical significance (or not) of these results AND that our data may need another method (e.g., non-parametric). cor() does have a way to run spearman instead. my_spearman_correlations &lt;- my_df %&gt;% select(SJCa,SJCh,DA) %&gt;% cor(method=&quot;spearman&quot;) print(my_spearman_correlations) ## SJCa SJCh DA ## SJCa 1.0000000 0.2727828 0.3148168 ## SJCh 0.2727828 1.0000000 0.2340174 ## DA 0.3148168 0.2340174 1.0000000 If we need p values though, we need to change to something else  corr.test my_results &lt;- corr.test(my_df$SJCa,my_df$DA) Then we can pull out results from this list or print it. Lets do both. print(my_results,short=FALSE) ## Call:corr.test(x = my_df$SJCa, y = my_df$DA) ## Correlation matrix ## [1] 0.33 ## Sample Size ## [1] 1852 ## Probability values adjusted for multiple tests. ## [1] 0 ## ## Confidence intervals based upon normal theory. To get bootstrapped values, try cor.ci ## raw.lower raw.r raw.upper raw.p lower.adj upper.adj ## NA-NA 0.29 0.33 0.37 0 0.29 0.37 my_results$r # correlation coefficient ## [1] 0.3342276 my_results$p # p-value ## [1] 1.433637e-49 Visually, we should be able to see this on a scatterplot. We are going to use qplot which stands for quickplot from within ggplot. It is useful and quicker for simple plotting than building up ggplot (though from the same package) we need to jitter my points (take geom=jitter out if you want to see why) qplot(SJCa,DA,data=my_df,geom=&quot;jitter&quot;) qqnorm(my_df$SJCa, frame = FALSE) qqline(my_df$SJCa, col = &quot;steelblue&quot;, lwd = 1.5) my_df %&gt;% ggplot(aes(x = SJCa)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Other functions we used today in class were describe() and also the q-q plot creation to investigate normality assumption copying syntax from the Field, Miles, &amp; Field book "],["week-5-regression-i.html", "Chapter5 Week 5: Regression I", " Chapter5 Week 5: Regression I Week 5 notes coming soon to an online book near you "],["week-6-regression-ii.html", "Chapter6 Week 6: Regression II", " Chapter6 Week 6: Regression II This is where the Week 6 magic happens "]]
